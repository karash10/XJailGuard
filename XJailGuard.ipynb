{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad576fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Installing all the necessary dependencies\n",
    "# ==========================================\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install shap --quiet\n",
    "!pip install numpy --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install ipython --quiet\n",
    "# Use bitsandbytes, not auto-gptq\n",
    "!pip install --upgrade transformers bitsandbytes --quiet\n",
    "!pip install indic-nlp-library jieba --quiet\n",
    "!pip install huggingface_hub --quiet\n",
    "!pip install gradio deep-translator langdetect --quiet\n",
    "!pip install rich --quiet # For console formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "# Import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import warnings\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import gradio as gr\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "jieba.setLogLevel(jieba.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894614c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# All the necessary models being declared\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "classification_model_name = \"Yashaswini21/multilingual_new1_continued\" # Using the model from previous script\n",
    "# --- USING BITSANDBYTES ---\n",
    "vicuna_model_name = \"TheBloke/Wizard-Vicuna-7B-Uncensored-HF\" # Use the HF model\n",
    "vicuna_tokenizer_name = \"lmsys/vicuna-7b-v1.5\" # Use the correct base tokenizer\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "output_filter_repo = \"khrshtt/XJailGuard\"\n",
    "output_filter_subfolder = \"output_filter\"\n",
    "multi_turn_subfolder = \"multi_turn\"\n",
    "use_auth = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# Load models (Only classifiers here, LLM is lazy-loaded)\n",
    "# =======================================================\n",
    "print(\"Loading classifier and filter models...\")\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(classification_model_name, **use_auth)\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(classification_model_name, **use_auth).to(device).eval()\n",
    "\n",
    "output_filter_tokenizer = AutoTokenizer.from_pretrained(output_filter_repo, subfolder=output_filter_subfolder, **use_auth)\n",
    "output_filter_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    output_filter_repo, subfolder=output_filter_subfolder, **use_auth\n",
    ").to(device).eval()\n",
    "\n",
    "multiturn_tokenizer = AutoTokenizer.from_pretrained(output_filter_repo, subfolder=multi_turn_subfolder, **use_auth)\n",
    "multiturn_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    output_filter_repo, subfolder=multi_turn_subfolder, **use_auth\n",
    ").to(device).eval()\n",
    "print(\"Classifier models loaded.\")\n",
    "\n",
    "vicuna_tokenizer = None\n",
    "vicuna_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "# word_tokenizer, predict_proba, multiturn_predict_proba, is_output_safe remain the same\n",
    "\n",
    "#This function takes a string (or a list of strings) and splits it into individual words or tokens based on the language detected.\n",
    "def word_tokenizer(text):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return [\"\"]\n",
    "    if isinstance(text, str):\n",
    "        if any(0x0900 <= ord(c) <= 0x097F for c in text):\n",
    "            return indic_tokenize.trivial_tokenize(text, lang=\"hi\")\n",
    "        elif any(0x4E00 <= ord(c) <= 0x9FFF for c in text):\n",
    "            tokens = list(jieba.cut(text))\n",
    "            return tokens or list(text)\n",
    "        else:\n",
    "            return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    elif isinstance(text, list):\n",
    "        return [word_tokenizer(t) for t in text]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type\")\n",
    "\n",
    "#This function takes text input and uses the multilingual classifier model (cls_model) to predict the probability of the input being \"benign\" or \"jailbreak\".\n",
    "def predict_proba(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, (list, tuple, np.ndarray)) and all(isinstance(item, list) for item in texts):\n",
    "        texts = [\" \".join(str(tok) for tok in tokens) for tokens in texts]\n",
    "    texts = [str(t) for t in texts if t and t.strip() != \"\"]\n",
    "    if not texts:\n",
    "        return np.zeros((1, cls_model.config.num_labels))\n",
    "    inputs = cls_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.nn.functional.softmax(cls_model(**inputs).logits, dim=-1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "#This function takes text input and uses the multi-turn classifier model (multiturn_model) to predict the probability of the input being safe or unsafe.\n",
    "def multiturn_predict_proba(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts = [str(t) for t in texts if t and t.strip() != \"\"]\n",
    "    if not texts:\n",
    "        return np.zeros((0, multiturn_model.config.num_labels)) # Return shape (0, N)\n",
    "    inputs = multiturn_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.nn.functional.softmax(multiturn_model(**inputs).logits, dim=-1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "#This function checks if the output text is safe using the output filter model (output_filter_model).\n",
    "def is_output_safe(text):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [t for t in texts if t and t.strip() != \"\"]\n",
    "    if not texts:\n",
    "        return True, \"benign\", np.array([])\n",
    "    inputs = output_filter_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = torch.nn.functional.softmax(output_filter_model(**inputs).logits, dim=-1)\n",
    "    pred = torch.argmax(probs, dim=-1).item()\n",
    "    label = output_filter_model.config.id2label[pred]\n",
    "    return pred == 0, label, probs.cpu().numpy()[0] # pred==0 is 'benign'\n",
    "\n",
    "# This loads the LLM for the chat assistant\n",
    "def generate_llm_response(prompt):\n",
    "    global vicuna_model, vicuna_tokenizer\n",
    "    if vicuna_model is None or vicuna_tokenizer is None:\n",
    "        print(\"\\nLoading 4-bit Vicuna model with bitsandbytes... (This may take a moment)\")\n",
    "        gr.Info(\"Loading 4-bit LLM with bitsandbytes... UI might be slow.\") # Gradio info\n",
    "        # 1. Define the 4-bit quantization config\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        # 2. Load the -HF model with this config\n",
    "        vicuna_model = AutoModelForCausalLM.from_pretrained(\n",
    "            vicuna_model_name,      # The -HF repo\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # 3. Load the correct tokenizer\n",
    "        vicuna_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            vicuna_tokenizer_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        vicuna_model.eval()\n",
    "        print(\"âœ… Vicuna model loaded successfully.\\n\")\n",
    "        gr.Info(\"LLM loaded successfully.\")\n",
    "\n",
    "    chat_prompt = f\"### Human: {prompt.strip()}\\n### Assistant:\"\n",
    "    inputs = vicuna_tokenizer(chat_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = vicuna_model.generate(\n",
    "            **inputs, max_new_tokens=300, temperature=0.8, top_p=0.95, do_sample=True,\n",
    "            repetition_penalty=1.15, eos_token_id=vicuna_tokenizer.eos_token_id,\n",
    "            pad_token_id=vicuna_tokenizer.pad_token_id\n",
    "        )\n",
    "    decoded = vicuna_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"### Assistant:\")[-1].strip() if \"### Assistant:\" in decoded else decoded.strip()\n",
    "\n",
    "# the filters used around the LLM\n",
    "def input_filter_func(texts):\n",
    "    probs = predict_proba(texts)[0]\n",
    "    pred_idx = np.argmax(probs)\n",
    "    return {\"label\": \"jailbreak\" if pred_idx == 1 else \"benign\", \"probs\": probs, \"pred_idx\": pred_idx}\n",
    "\n",
    "def output_filter_func(texts):\n",
    "    safe, label, probs = is_output_safe(texts[0])\n",
    "    return {\"label\": \"benign\" if safe else \"jailbreak\", \"probs\": probs, \"pred_idx\": np.argmax(probs)}\n",
    "\n",
    "# =========================\n",
    "# SHAP explainers\n",
    "# =========================\n",
    "print(\"Initializing SHAP explainers...\")\n",
    "single_turn_masker = shap.maskers.Text(tokenizer=cls_tokenizer, mask_token=\"...\")\n",
    "single_turn_explainer = shap.Explainer(predict_proba, masker=single_turn_masker, algorithm=\"partition\")\n",
    "\n",
    "multiturn_masker = shap.maskers.Text(tokenizer=multiturn_tokenizer, mask_token=\"...\")\n",
    "multiturn_explainer = shap.Explainer(multiturn_predict_proba, masker=multiturn_masker, algorithm=\"partition\")\n",
    "print(\"SHAP explainers ready.\")\n",
    "\n",
    "\n",
    "class ConversationSafeAgent:\n",
    "    def _init_(self, input_filter, output_filter, single_turn_explainer, multiturn_explainer, model_func, max_context_turns=2):\n",
    "        self.memory = []  # Internal memory\n",
    "        self.max_context_turns = max_context_turns  # How many past user/assistant pairs for multi-turn check\n",
    "        self.input_filter = input_filter\n",
    "        self.output_filter = output_filter\n",
    "        self.single_turn_explainer = single_turn_explainer\n",
    "        self.multiturn_explainer = multiturn_explainer\n",
    "        self.model_func = model_func\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clears the agent's internal conversation memory.\"\"\"\n",
    "        self.memory = []\n",
    "\n",
    "    # Returns list of lists for Gradio DataFrame\n",
    "    def explain_shap_values(self, explainer, text, pred_index=None, top_k=7, original_text=None):\n",
    "        \"\"\"\n",
    "        Computes SHAP values for the given text and returns top_k impactful tokens.\n",
    "        original_text: the text in the original language (for multi-turn display)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            shap_vals_list = explainer([text])\n",
    "            if not shap_vals_list:\n",
    "                return [[\"Error\", \"No SHAP output\"]]\n",
    "\n",
    "            shap_vals = shap_vals_list[0]\n",
    "            tokens = shap_vals.data\n",
    "            values = shap_vals.values\n",
    "\n",
    "            # If multi-class, select the column\n",
    "            if values.ndim != 1 and pred_index is not None and values.shape[1] > pred_index:\n",
    "                values = values[:, pred_index]\n",
    "            elif values.ndim != 1:\n",
    "                values = values.mean(1)\n",
    "\n",
    "            # Use original tokens if provided (for multi-turn)\n",
    "            if original_text is not None:\n",
    "                try:\n",
    "                    orig_tokens = word_tokenizer(original_text)\n",
    "                    # Flatten if nested list\n",
    "                    if any(isinstance(t, list) for t in orig_tokens):\n",
    "                        orig_tokens = [t for sub in orig_tokens for t in sub]\n",
    "                except:\n",
    "                    orig_tokens = tokens\n",
    "            else:\n",
    "                orig_tokens = tokens\n",
    "\n",
    "            # Pair tokens with SHAP values\n",
    "            valid_pairs = [(str(t), float(v)) for t, v in zip(orig_tokens, values) if str(t).strip()]\n",
    "            # Sort by absolute impact descending\n",
    "            pairs = sorted(valid_pairs, key=lambda x: abs(x[1]), reverse=True)[:top_k]\n",
    "\n",
    "            return [[token, f\"{impact:+.5f}\"] for token, impact in pairs]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP explanation error: {e}\")\n",
    "            return [[\"Error\", str(e)]]\n",
    "\n",
    "    # Takes chat_history, returns (response, label, explanation_data)\n",
    "    def classify_and_generate(self, user_prompt, chat_history):\n",
    "        if not user_prompt or not user_prompt.strip():\n",
    "            return None, 'Invalid Input', None\n",
    "\n",
    "        # 1. Single-turn check\n",
    "        input_pred = self.input_filter([user_prompt])\n",
    "        if input_pred['label'] == 'jailbreak':\n",
    "            exp_data = self.explain_shap_values(\n",
    "                self.single_turn_explainer, \n",
    "                user_prompt, \n",
    "                input_pred.get('pred_idx', 1), \n",
    "                original_text=user_prompt\n",
    "            )\n",
    "            return None, 'Unsafe Input', exp_data\n",
    "\n",
    "        # 2. Language detection + translation\n",
    "        try:\n",
    "            user_lang = detect(user_prompt)\n",
    "        except Exception:\n",
    "            user_lang = \"en\"\n",
    "        try:\n",
    "            translated_prompt = GoogleTranslator(source='auto', target='en').translate(user_prompt) if user_lang != \"en\" else user_prompt\n",
    "        except Exception as e:\n",
    "            print(f\"Input translation failed: {e}\")\n",
    "            translated_prompt = user_prompt\n",
    "            user_lang = \"en\"\n",
    "        if not translated_prompt:\n",
    "            translated_prompt = user_prompt\n",
    "\n",
    "        # 3. Multi-turn check using internal memory\n",
    "        context_prompts = self.memory[-(self.max_context_turns * 2):] + [translated_prompt]\n",
    "        full_conversation_text = \"\\n\".join(context_prompts)\n",
    "\n",
    "        if full_conversation_text.strip():\n",
    "            multi_turn_probs_list = multiturn_predict_proba(full_conversation_text)\n",
    "            if multi_turn_probs_list.shape[0] > 0:\n",
    "                multi_turn_pred_idx = np.argmax(multi_turn_probs_list[0])\n",
    "                if multi_turn_pred_idx == 1:  # 1 is 'jailbreak'\n",
    "                    exp_data = self.explain_shap_values(\n",
    "                        self.multiturn_explainer,\n",
    "                        full_conversation_text,\n",
    "                        1,\n",
    "                        original_text=\"\\n\".join(self.memory[-(self.max_context_turns * 2):] + [user_prompt])\n",
    "                    )\n",
    "                    return None, 'Unsafe Conversation', exp_data\n",
    "\n",
    "        # 4. Generate response\n",
    "        response_en = self.model_func(translated_prompt)\n",
    "        if not response_en or not response_en.strip():\n",
    "            print(\"Warning: LLM generated empty response.\")\n",
    "            final_response = \"I couldn't generate a response for that prompt.\"\n",
    "            self.memory.extend([translated_prompt, \"Failed to generate response.\"])\n",
    "            return final_response, 'Safe', None\n",
    "\n",
    "        # 5. Output filter\n",
    "        safe, _, _ = is_output_safe(response_en)\n",
    "        if not safe:\n",
    "            exp_data = self.explain_shap_values(\n",
    "                self.single_turn_explainer,\n",
    "                response_en,\n",
    "                1,\n",
    "                original_text=user_prompt\n",
    "            )\n",
    "            return None, 'Unsafe Output', exp_data\n",
    "\n",
    "        # 6. Translate back to original language\n",
    "        try:\n",
    "            final_response = GoogleTranslator(source='en', target=user_lang).translate(response_en) if user_lang != \"en\" else response_en\n",
    "        except Exception as e:\n",
    "            print(f\"Return translation failed: {e}\")\n",
    "            final_response = response_en\n",
    "        if not final_response:\n",
    "            final_response = response_en\n",
    "\n",
    "        # --- Update agent memory on success ---\n",
    "        self.memory.extend([translated_prompt, response_en])\n",
    "\n",
    "        return final_response, 'Safe', None\n",
    "\n",
    "# =========================\n",
    "# Initialize agent\n",
    "# =========================\n",
    "print(\"Initializing ConversationSafeAgent...\")\n",
    "agent = ConversationSafeAgent(\n",
    "    input_filter=input_filter_func,\n",
    "    output_filter=output_filter_func,\n",
    "    single_turn_explainer=single_turn_explainer,\n",
    "    multiturn_explainer=multiturn_explainer,\n",
    "    model_func=generate_llm_response, # Pass the function\n",
    "    max_context_turns=2 # Number of user/bot pairs\n",
    ")\n",
    "print(\"Agent is ready.\")\n",
    "\n",
    "# =========================\n",
    "# Gradio App\n",
    "# =========================\n",
    "def handle_chat(user_input, history):\n",
    "    \"\"\"\n",
    "    Gradio chat function.\n",
    "    Takes user input and current history (for display).\n",
    "    Calls the agent (which now uses its internal memory).\n",
    "    Returns updated history for display and explanation data.\n",
    "    \"\"\"\n",
    "    history = history or [] # Ensure history is a list for display\n",
    "\n",
    "    # Pass Gradio history for context IF NEEDED by agent, but agent uses self.memory now\n",
    "    # For this version, agent uses self.memory, so we pass an empty list or None if not needed\n",
    "    response, label, explanation_data = agent.classify_and_generate(user_input, []) # Pass empty list as classify_and_generate uses self.memory now\n",
    "\n",
    "    explanation_df = pd.DataFrame(columns=[\"Token\", \"Impact\"]) # Default empty DataFrame\n",
    "    block_message = None\n",
    "\n",
    "    if response: # Success\n",
    "        history.append((user_input, response))\n",
    "    else: # Blocked\n",
    "        block_message = f\"âš  Response Blocked: {label}\"\n",
    "\n",
    "        # Clear history AND agent memory \n",
    "        unsafe_labels = ['Unsafe Input', 'Unsafe Conversation', 'Unsafe Output']\n",
    "        if label in unsafe_labels:\n",
    "            print(f\"Block detected ({label}). Clearing Gradio history AND agent memory.\") # Log update\n",
    "            history = [] # Reset the Gradio display history list\n",
    "            agent.clear_memory() # Call the agent's internal memory clear method\n",
    "            block_message += \" (Chat history and agent memory cleared)\" # Update message\n",
    "\n",
    "        # Append the user message and the (potentially updated) block message AFTER clearing\n",
    "        history.append((user_input, block_message))\n",
    "\n",
    "        if explanation_data:\n",
    "            explanation_df = pd.DataFrame(explanation_data, columns=[\"Token\", \"Impact\"])\n",
    "\n",
    "    return history, explanation_df\n",
    "\n",
    "# =========================\n",
    "# Gradio UI Layout\n",
    "# =========================\n",
    "print(\"\\nBuilding Gradio Interface...\")\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Secure Multilingual AI Assistant\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(label=\"Conversation\", height=500)\n",
    "            msg = gr.Textbox(label=\"Enter prompt\", placeholder=\"Type your message here...\", lines=3)\n",
    "            submit_btn = gr.Button(\"Send\")\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Analysis of Unsafe Content\")\n",
    "            gr.Markdown(\"The following tokens were identified as contributing to a potential jailbreak or policy violation:\")\n",
    "            explanation_df_output = gr.DataFrame(headers=[\"Token\", \"Impact\"], datatype=[\"str\", \"str\"], label=\"\")\n",
    "\n",
    "    # --- Event Handlers ---\n",
    "    # 1. Handle message submission (Enter key)\n",
    "    msg.submit(handle_chat, [msg, chatbot], [chatbot, explanation_df_output])\n",
    "    msg.submit(lambda: \"\", None, msg) # Clear input textbox after submit\n",
    "\n",
    "    # 2. Handle button click\n",
    "    submit_btn.click(handle_chat, [msg, chatbot], [chatbot, explanation_df_output])\n",
    "    submit_btn.click(lambda: \"\", None, msg) # Clear input textbox after click\n",
    "\n",
    "print(\"Launching Gradio Interface...\")\n",
    "# share=True is needed for Colab public link\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
